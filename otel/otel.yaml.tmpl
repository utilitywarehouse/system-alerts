# PROMETHEUS RULES
# DO NOT REMOVE line above, used in `pre-commit` hook

groups:
  # Infra
  - name: otel-infra
    rules:
      - alert: VolumeAvailableCapacity
        expr: 1 - (kubelet_volume_stats_available_bytes{namespace="otel"} / kubelet_volume_stats_capacity_bytes{namespace="otel"}) > 0.8
        labels:
          team: otel
        annotations:
          summary: "{{$labels.namespace}}/{{$labels.persistentvolumeclaim}} has had more than 80% of its capacity utilised."
      - alert: CPUThrottlingHigh
        expr: sum(increase(container_cpu_cfs_throttled_periods_total{namespace="otel"}[5m])) by (container, pod, namespace) / sum(increase(container_cpu_cfs_periods_total{namespace="otel"}[5m])) by (container, pod, namespace) > (80/100)
        for: 10m
        labels:
          team: otel
        annotations:
          summary: "{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}."
          runbook: https://github.com/utilitywarehouse/dev-enablement-docs/blob/main/runbooks/otel.md#cputhrottlinghigh

      - alert: DeploymentReplicasMismatch
        expr: kube_deployment_spec_replicas{namespace="otel"} != kube_deployment_status_replicas_available{namespace="otel"}
        for: 5m
        labels:
          team: otel
        annotations:
          summary: "{{$labels.namespace}}/{{$labels.deployment}} doesn't have as many pods as the deployment requests."
      - alert: PodOOMing
        expr: rate(kube_pod_container_status_terminated_reason{reason="OOMKilled",namespace="otel"}[30m]) != 0
        labels:
          team: otel
        annotations:
          summary: "{{$labels.namespace}}/{{$labels.pod}} pod is OOMing and should be investigated"
      - alert: JobFailure
        expr: sum(increase(kube_job_status_failed{namespace="otel"}[10m])) by (job_name) != 0
        for: 1m
        labels:
          team: otel
        annotations:
          summary: "{{$labels.job_name}} failed to complete."

  # Collector - Tracing
  - name: otel-collector
    rules:
      - alert: ReceiverRefusedSpans
        expr: rate(otelcol_receiver_refused_spans{kubernetes_namespace="otel"}[5m]) > 0
        for: 5m
        labels:
          team: otel
        annotations:
          summary: "{{ $labels.pod }} collector is refusing trace spans from clients"
      - alert: ProcessorDroppedSpans
        expr: rate(otelcol_processor_dropped_spans{kubernetes_namespace="otel"}[5m]) > 0
        for: 5m
        labels:
          team: otel
        annotations:
          summary: "{{ $labels.pod }} collector is dropping trace spans"
      - alert: ExporterFailedToSendSpans
        expr: rate(otelcol_exporter_send_failed_spans{kubernetes_namespace="otel"}[5m]) > 0
        for: 5m
        labels:
          team: otel
        annotations:
          summary: "{{ $labels.pod }} collector failing to export trace spans"
      - alert: ExporterFailedToEnqueueSpans
        expr: rate(otelcol_exporter_enqueue_failed_spans{kubernetes_namespace="otel"}[5m]) > 0
        for: 5m
        labels:
          team: otel
        annotations:
          summary: "{{ $labels.pod }} collector failing to enqueue trace spans. Check the logs"
  - name: otel-keda-hpa
    rules:
      - alert: HeadCollectorScaledMax
        expr: kube_horizontalpodautoscaler_status_current_replicas{namespace="otel",horizontalpodautoscaler="keda-hpa-otel-collector-scaling"}==kube_horizontalpodautoscaler_spec_max_replicas{namespace="otel",horizontalpodautoscaler="keda-hpa-otel-collector-scaling"}
        for: 2h
        labels:
          team: otel
        annotations:
          summary: 'The head otel collector has been scaled to maximum replicas for 2h'
          runbook: <https://wiki.uw.systems/posts/open-telemetry-collector-tempo-runbooks-9otrgeem#head-collector-scaled-max|link>
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/BKf2sowmj2/opentelemetry-collector?from=now-2h&to=now&timezone=utc&refresh=1m|link>
          priority: P3

