# PROMETHEUS RULES
# DO NOT REMOVE line above, used in `pre-commit` hook

groups:
  - name: team_detection
    rules:
      # used as: `<metric_with_dbidentifier_label> + on (dbidentifier) group_left (team) uw_rds_owner_team` as the value is always 0
      - record: uw_rds_owner_team
        expr: 'sum by(dbidentifier,team) (label_replace(rds_instance_tags{tag_owner!=""}, "team", "$1", "tag_owner", "(.*)"))'

    #   based on https://github.com/qonto/database-monitoring-framework/blob/main/charts/prometheus-rds-alerts/values.yaml
    #   and AWS https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Best_Practice_Recommended_Alarms_AWS_Services.html#RDS
  - name: RDS
    rules:
      - alert: RDSFreeableMemory
        expr: |
          (max by (aws_account_id, aws_region, dbidentifier) (rds_freeable_memory_bytes{}) * 100 / on(aws_account_id, aws_region, dbidentifier) 
               (max by (instance_class) (rds_instance_memory_bytes{}) * on (instance_class) group_right() max by (aws_account_id, aws_region, dbidentifier, instance_class) (rds_instance_info{})) 
          < 20 ) + on (dbidentifier) group_left (team) uw_rds_owner_team
        for: 10m
        labels:
          alerttype: stock
          alertgroup: RDS
        annotations:
          summary: 'RDS instance {{$labels.dbidentifier}} has {{ printf "%.0f" $value }}% freeable memory'
          impact: "Running out of memory can result in rejected connections"
          link: "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Best_Practice_Recommended_Alarms_AWS_Services.html#RDS"
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-30&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          qonto_runbook: "<https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSMemoryUtilization/|link>"
          action: |
            Low freeable memory means that there is a spike in database connections or that your instance may be under high memory pressure. 
            Check for memory pressure by monitoring the SwapUsage in addition to FreeableMemory. 
            If the instance memory consumption is frequently too high, this indicates that you should check your workload or upgrade your instance class.
            NOTE: `db.t4g.micro` is constantly under memory pressure, so it's usable only for testing that your connection setup works, but it's not suitable for dev usage.
            Consider upgrade at least to `db.t4g.small`.

      - alert: RDSCPUUtilization
        expr: '(max by (aws_account_id, aws_region, dbidentifier) (rds_cpu_usage_percent_average) > 85) + on (dbidentifier) group_left (team) uw_rds_owner_team'
        for: 10m
        labels:
          alerttype: stock
          alertgroup: RDS
        annotations:
          summary: "RDS instance {{$labels.dbidentifier}} has high CPU utilisation"
          impact: "High CPU utilization can lead to very high response time and time-outs"
          link: "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Best_Practice_Recommended_Alarms_AWS_Services.html#RDS"
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-27&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          qonto_runbook: "<https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSCPUUtilization/|link>"
          action: |
            CPU utilization measures non-idle time.
            Consider using Enhanced Monitoring or Performance Insights to review which wait time is consuming the most of the CPU time (guest, irq, wait, nice, and so on). 
            Then evaluate which queries consume the highest amount of CPU. 
            If you can't tune your workload, consider moving to a larger DB instance class.
      # Merged the AWS DBload alert & qonto NonCPUUtilisation into this one.
      # Diverged from the qonto alert by using as a threshold the number of vCPUS,as AWS recommends, rather than multiplying with 4.
      # We might need to adjust it to use multiplication, if we see it triggering a lot, based on the response from Qonto: https://github.com/qonto/database-monitoring-framework/issues/48
      - alert: RDSDBLoad
        expr: |
          max by (aws_account_id, aws_region, dbidentifier) (rds_dbload_average)
          > on(aws_account_id, aws_region, dbidentifier) (
          max by (instance_class) (rds_instance_vcpu_average{}) * on (instance_class) group_right() max by (aws_account_id, aws_region, dbidentifier, instance_class) (rds_instance_info{})
          ) + on (dbidentifier) group_left (team) uw_rds_owner_team
        for: 10m
        labels:
          alerttype: stock
          alertgroup: RDS
        annotations:
          summary: 'RDS instance {{$labels.dbidentifier}} has high DB load: {{ printf "%.0f" $value }}'
          impact: "Performance is degraded because some queries could not be executed."
          link: "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Best_Practice_Recommended_Alarms_AWS_Services.html#RDS"
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-26&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          qonto_runbook: "<https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSNonCPUUtilization/|link>"
          perf_insides_link: "<https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.Components.html#USER_PerfInsights.UsingDashboard.Components.AvgActiveSessions.dims|link>"
          action: |
            If the number of processes exceed the number of vCPUs, the processes start queuing. When the queuing increases, the performance is impacted. 
            Check the linked dashboard as the are 2 causes:
            1. The DB load is caused by `CPU execution`: you can monitor CPUUtilization, DBLoadCPU and queued tasks in Performance Insights/Enhanced Monitoring. 
               You might want to throttle connections to the instance, tune any SQL queries with a high CPU load, or consider a larger instance class.
            2. The DB load is caused by `Non CPU execution`:             
              This situation usually occurs when SQL queries are blocked by software reason (e.g. Locks) or hardware saturation (e.g. IOPS)
              Identify the waits type of active queries on RDS Performance insights.
              Select `Top waits` and `Top database` to quickly identify wait reason and database.
              Mitigation:
              a. If Lock:relation, identify and fix the lock reason
                 For other wait type, looks at https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/PostgreSQL.Tuning.html
              b. Kill the SQL queries

      - alert: RDSSwapUtilization
        expr: (max by (aws_account_id, aws_region, dbidentifier) (delta(rds_swap_usage_bytes{}[1h])) / 1024 / 1024 >= 20) + on (dbidentifier) group_left (team) uw_rds_owner_team
        for: 2m
        annotations:
          summary: "{{ $labels.dbidentifier }} SWAP utilization is high"
          impact: "Performance could be degraded"
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-32&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          qonto_runbook: "<https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSSwapUtilization/|link>"
          action: |
            1. Check memory usage over last weeks to identify if server is missing memory
            2. Check if there long running PostgreSQL clients that donâ€™t execute SQL queries for a while on live dashboard:
              ```SQL
              SELECT
                pid,
                usename,
                datname,
                application_name,
                age(now(), query_start) as last_query_age,
                age(now(), backend_start) as backend_age,
                backend_start,
                query_start last_query,
                left(query, 60) query
              FROM pg_stat_activity
              WHERE query_start is not null
              AND pid != pg_backend_pid()
              AND usename != 'rdsrepladmin'
              AND state != 'active'
              AND query not like 'START_REPLICATION %'
              AND query_start < NOW() - INTERVAL '24 HOURS'
              ORDER by query_start asc;
              ```
            Mitigation
            1. Reduce number of concurrent connections on the server
            2. Increase RDS instance type to have more memory

      - alert: RDSIOPSUtilization
        expr: (max by (aws_account_id, aws_region, dbidentifier) ((rds_read_iops_average{} + rds_write_iops_average{}) * 100 / rds_max_disk_iops_average{}) > 80)  + on (dbidentifier) group_left (team) uw_rds_owner_team
        for: 10m
        annotations:
          summary: '{{ $labels.dbidentifier }} uses {{ printf "%.0f" $value }}% of its disk IOPS'
          impact: "Performance could be degraded"
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-29&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          qonto_runbook: "<https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSIOPSUtilization/|link>"
          action: |
            Open RDS Performance insights to identify IOPS-intensive queries.
            Mitigation:
              1. Kill SQL queries that generate intensive IOPS
              2. Try to improve the query
              3. Increase provisioned IOPS if possible (gp3 or io1 storage class). Be aware that some instances have IOPS limits.

      - alert: RDSPostgreSQLMaximumUsedTransaction
        expr: (max by (aws_account_id, aws_region, dbidentifier) (rds_maximum_used_transaction_ids_average) > (2^32) * 0.5) + on (dbidentifier) group_left (team) uw_rds_owner_team # 50% of the max transactions limit
        for: 5m
        annotations:
          summary: "{{ $labels.dbidentifier }} is using {{ $value }} transaction IDs on 4 billions hard limit"
          impact: "PostgreSQL will shut down and refuse to start any new transactions once there are fewer than 3 million transactions left until wraparound"
          link: "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Best_Practice_Recommended_Alarms_AWS_Services.html#RDS"
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-37&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          qonto_runbook: "<https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSPostgreSQLMaximumUsedTransaction/|link>"
          action: |
            Check the <https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSPostgreSQLMaximumUsedTransaction/|Qonto runbook> to deal with this.
            AWS also refers to these resources for troubleshooting this: <https://aws.amazon.com/blogs/database/implement-an-early-warning-system-for-transaction-id-wraparound-in-amazon-rds-for-postgresql/|guide> 
            and <https://aws.amazon.com/blogs/database/understanding-autovacuum-in-amazon-rds-for-postgresql-environments/|autovacuum concepts>

      - alert: RDSDiskSpaceLimit
        # Autoscaling kicks in at least at < 10%
        expr: (max by (aws_account_id, aws_region, dbidentifier) (rds_free_storage_bytes{} * 100 / rds_allocated_storage_bytes{}) < 10) + on (dbidentifier) group_left (team) uw_rds_owner_team
        for: 20m
        annotations:
          summary: '{{ $labels.dbidentifier }} has {{ printf "%.2g" $value }}% free disk space'
          impact: "The PostgreSQL instance will stop to prevent data corruption if no more disk space is available."
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-33&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          qonto_runbook: "<https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSDiskSpaceLimit/|link>"
          action: |
            Check that autoscaling is turned on for your instance.
            See <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.Autoscaling.html|AWS docs>.
            Check the <https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSDiskSpaceLimit/|Qonto runbook> for details on how to reclaim space.
          
      - alert: RDSDiskSpacePrediction
        # trigger only for instances without autoscaling enabled. 
        # using `unless on(dbidentifier) rds_max_allocated_storage_bytes{}` in the expression as only instances with autoscaling enabled have the metric rds_max_allocated_storage_bytes
        expr: |
          (predict_linear(min by (aws_account_id, aws_region, dbidentifier) (rds_free_storage_bytes{})[30m:], 3600 * 4) < 1)
          unless on(dbidentifier) rds_max_allocated_storage_bytes{}
          + on (dbidentifier) group_left (team) uw_rds_owner_team
        for: 10m
        annotations:
          summary: "{{ $labels.dbidentifier }} will run out of disk space in 4 hours"
          impact: "The PostgreSQL instance will stop to prevent data corruption if no more disk space is available."
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-33&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          qonto_runbook: "<https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSDiskSpacePrediction/|link>"
          action: |
            Turn on storage autoscaling for your instance.
            See <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.Autoscaling.html|AWS docs>.
            Check the <https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSDiskSpacePrediction/|Qonto runbook> for details on how to action further.

      - alert: RDSStorageAutoscaling
        expr: (max by (aws_account_id, aws_region, dbidentifier) (rds_allocated_storage_bytes{} * 100 / rds_max_allocated_storage_bytes{}) > 80) + on (dbidentifier) group_left (team) uw_rds_owner_team
        annotations:
          summary: "{{ $labels.dbidentifier }} allocated storage reached 80% of the maximum storage threshold for autoscaling"
          impact: "RDS instance may not be able to autoscale storage"
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-41&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          rds-autoscaling: <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.Autoscaling.html|link>
          action: |
            Increase the the maximum storage threshold for autoscaling by updating the property `max_allocated_storage`
            in the terraform project for your instance and perform terraform apply.

      - alert: RDSUnappliedParameters
        expr: (max by (aws_account_id, aws_region, dbidentifier) (rds_instance_info{pending_modified_values="true"}) > 0) + on (dbidentifier) group_left (team) uw_rds_owner_team
        for: 1h
        annotations:
          summary: "{{ $labels.dbidentifier }} has unapplied parameters"
          impact: "RDS instance is running with outdated configuration"
          qonto_runbook: "<https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSDiskSpacePrediction/|link>"
          action: |
            Apply the new parameters by restarting the instance
            Find a suitable time slot for this, as it will cause a momentary outage.
            Check the <https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSDiskSpacePrediction/|Qonto runbook>
            for details on tracking what changed.
