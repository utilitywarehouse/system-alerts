# PROMETHEUS RULES
# DO NOT REMOVE line above, used in `pre-commit` hook

groups:
  - name: infra
    rules:
      - alert: KubeletCadvisorNotResponding
        expr: up{job="kubernetes-nodes"} != 1
        for: $NODE_ROLL_WINDOW
        labels:
          team: infra
        annotations:
          description:
            "{{ $labels.instance }} ({{ $labels.role }}) has been down for
            over $NODE_ROLL_WINDOW"
          summary: "Kubernetes node is down for over $NODE_ROLL_WINDOW"
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/VAE0wIcik/kubernetes-pod-resources?orgId=1&refresh=1m&var-instance={{ $labels.instance }}&var-namespace=All&var-app=All&var-app_kubernetes_io_name=All|link>
      - alert: KubernetesCACertExpiringSoon
        expr: ca_cert_expiry_timestamp{app="kube-ca-cert-server"} - time() < 86400 * 2
        labels:
          team: infra
          priority: P1
        annotations:
          summary: "Kubernetes CA Certificate {{$labels.kubernetes_cluster}} is expiring in < 2 days"
          impact: "{{$labels.kubernetes_cluster}} will have TLS errors if not renewed."
          action: "Renew certificate for {{$labels.kubernetes_cluster}}"
      - alert: KubernetesApiDown
        expr: up{service="kubernetes"} != 1
        for: 1m
        labels:
          team: infra
        annotations:
          description: The kubernetes API is down within the cluster.
          summary: Kubernetes API server is down
      - alert: KubernetesSchedulerDown
        expr: up{service="kube-scheduler"} != 1
        for: 5m
        labels:
          team: infra
        annotations:
          description: "{{ $labels.pod }} scheduler has been down for several minutes."
          summary: Kubernetes scheduler {{ $labels.pod }} is down
      - alert: KubeStateMetricsAbsent
        expr: absent(kube_state_metrics_build_info)
        for: 5m
        labels:
          team: infra
        annotations:
          description: "A number of our critical alerts depend on the metrics produced by kube-state-metrics."
          summary: Metrics from kube-state-metrics are absent
      - alert: NodeNotReady
        expr:
          count(kube_node_status_condition{condition="Ready", status!="true"} != 0) > 0
        for: $NODE_ROLL_WINDOW
        labels:
          team: infra
        annotations:
          description:
            One or more of the worker nodes in the cluster is marked as Not
            Ready.
          summary: Kubernetes node is Not Ready
      - alert: NodeNotReadyAlertInhibitor
        keep_firing_for: 2m
        expr:
          count(kube_node_status_condition{condition="Ready", status!="true"} != 0) > 0
        labels:
          team: infra
      - alert: NodeStuckRebooting
        expr:
          sum(kured_reboot_required) > 0 and sum(kured_reboot_required) - sum(kured_reboot_required offset 30m) == 0 and ON() hour() > 6 < 18
        labels:
          team: infra
        annotations:
          description: Workers require rebooting but no reboots have occurred in the last 30m
          summary: A worker appears to be stuck rebooting
      - alert: RebootRequiredAlertInhibitor
        expr:
          sum(kured_reboot_required) > 0
        labels:
          team: infra
      - alert: NodeUnschedulable
        expr: count(kube_node_spec_unschedulable == 1) > 0
        for: 1h
        labels:
          team: infra
        annotations:
          description: One or more of the nodes in the cluster is marked as unschedulable.
          summary: Kubernetes cluster has unschedulable node(s)
      - alert: NodeNoDiskSpace
        expr: kube_node_status_condition{condition="OutOfDisk", status="true"} != 0
        for: 2m
        labels:
          team: infra
        annotations:
          description: "{{ $labels.node }} is reporting that it is out of disk space."
          summary: node {{ $labels.node }} has no disk space
#      - alert: KubernetesHighNodeMemory
#        expr: ((sum by (instance, kubernetes_cluster) (node_memory_MemTotal_bytes{job="node-exporter"}) - ((sum by (instance, kubernetes_cluster) (node_memory_MemAvailable_bytes{job="node-exporter"})))) / sum by (instance, kubernetes_cluster) (node_memory_MemTotal_bytes{job="node-exporter"}) * 100 ) > 95
#        for: 5m
#        labels:
#          team: infra
#        annotations:
#          description:
#            "{{ $labels.kubernetes_cluster }} / {{ $labels.instance }} is reporting mem usage higer than 95%"
#          summary: Kubernetes node high memory
#          value: "{{ $value }}"
#      - alert: KubernetesHighNodeCPU
#        expr: (( sum by (instance, kubernetes_cluster)(rate(node_cpu_seconds_total{mode!='idle',job="node-exporter"}[10m])) / sum by (instance, kubernetes_cluster)(rate(node_cpu_seconds_total{job="node-exporter"}[10m]))) * 100 ) > 95
#        for: 5m
#        labels:
#          team: infra
#        annotations:
#          description:
#            "{{ $labels.kubernetes_cluster }} / {{ $labels.instance }} is reporting cpu usage higher than 95%"
#          summary: Kubernetes node high cpu
#          value: "{{ $value }}"
      # Non-kube targets have their own dedicated alerts
      - alert: NodeExporterDown(kube)
        # Joining with kube_pod_info to get the pod name of the exporter, to enable the loki link
        expr: up{job="node-exporter-static", node!=""} * on (node) group_left(pod) kube_pod_info{created_by_name="node-exporter"} == 0
        for: 5m
        labels:
          team: infra
        annotations:
          summary: "{{ $labels.node }} prometheus scrapes are failing on a discovered node"
          impact: "Node may be misbehaving"
          action: "Check if node has a read-only filesystem, which is a common cause for exporter failures (check events in `kubectl describe node`). Otherwise check node-exporter logs (link below)"
          logs: <https://grafana.$ENVIRONMENT.aws.uw.systems/explore?left=["now-1h","now","Loki",{"expr":"{cloud_provider=\"{{$labels.cloud_provider}}\",kubernetes_pod_name=\"{{$labels.pod}}\"}"}]|link>
      - alert: CfsslDown
        expr: probe_success{job="cfssl-probe"} == 0 or absent(probe_success{job="cfssl-probe"})
        for: 10m
        labels:
          team: infra
        annotations:
          summary: "{{ $labels.instance }} reports down more than 10 minutes."
      - alert: VolumeDiskUsage
        expr: kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes * on(namespace) group_left kube_namespace_labels{label_uw_systems_owner="system"} > 0.9
        for: 5m
        labels:
          team: infra
        annotations:
          summary: "Volume {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} has less than 10% available capacity"
          impact: "Exhausting available disk space will most likely result in service disruption"
          action: "Investigate disk usage and adjust volume size if necessary."
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/919b92a8e8041bd567af9edab12c840c/kubernetes-persistent-volumes?orgId=1&var-datasource=default&var-cluster=${ENVIRONMENT}-${PROVIDER}&var-namespace={{ $labels.namespace }}&var-volume={{ $labels.persistentvolumeclaim }}|link>
      - alert: CertExpireK8SSidecarInjector
        expr: (probe_ssl_earliest_cert_expiry{job="k8s-sidecar-injector-tls-probe"} - time()) / 60 / 60 / 24 < 7 # Less then a week
        for: 5m
        labels:
          team: infra
        annotations:
          summary: "The k8s-sidecar-injector-webhook certificate will expire in < 7 days"
          impact: "APIServer will not be able to talk to k8s-sidecar-injector and applications will not have sidecars injected"
          action: "Short term: restart k8s-sidecar-injector Deployment. Long term: Make sure k8s-sidecar-injector reloads certificate/key when they change on disk"
          dashboard: <https://thanos-query-sys-prom.$ENVIRONMENT.$PROVIDER.uw.systems//graph?g0.expr=%28probe_ssl_earliest_cert_expiry%7Bjob%3D%22k8s-sidecar-injector-tls-probe%22%7D+-+time%28%29%29+%2F+60+%2F+60+%2F+24&g0.tab=0|link>
      - alert: AvailabilityZoneRunningOutOfMemory
        expr: avg(node_memory_working_set_bytes/on(node)(kube_node_status_capacity{resource="memory"} - on (node) node_eviction_threshold) * on(node) group_left(zone) kube_node_labels{role="worker"}) by (zone) > 0.90
        for: 6h
        labels:
          team: infra
        annotations:
          summary: "AZ {{ $labels.zone}} is running out of memory for pods"
          impact: "Nodes are about to get into 'MemoryPressure' state and start to evict pods."
          action: "Increase the number of nodes in the cluster."
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/Mig_eDNVz/kubernetes-cluster-utilization|link>
      - alert: AvailabilityZoneRunningOutOfAllocatableMemory
        expr: avg(kube:node:resource:requests{resource="memory"}/on(resource,node) avg(kube_node_status_allocatable{resource="memory"}) by (node,resource) * on(node) group_left(zone) kube_node_labels{role="worker"}) by (zone) > 0.9
        for: 6h
        labels:
          team: infra
        annotations:
          summary: "AZ {{ $labels.zone}} is running out of allocatable memory"
          impact: "Pods may not find room to get scheduled."
          action: "Review workloads' resource reservations and adjust them."
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/Mig_eDNVz/kubernetes-cluster-utilization AND https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/SAY0wIcik/kubernetes-resource-provisioning|link>
  - name: kyverno
    rules:
      - alert: KyvernoBackgroundCheckRuleFailure
        expr: increase(kyverno_policy_results_total{rule_execution_cause="background_scan", rule_result="fail"}[1h]) > 0
        labels:
          team: infra
        annotations:
          summary: "Kyverno policy: {{ $labels.policy_name }} rule: {{ $labels.rule_name }} is failing in {{ $labels.resource_namespace }} namespace."
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/Rg8lWBG7k/kyverno|link>
          description: |
            Background checks for rule: {{ $labels.rule_name }} of policy: {{ $labels.policy_name }}
            report failures under {{ $labels.resource_namespace }} namespace.

            To get more info about the issue check the namespace events with the following command:
            `kubectl --context={{ $labels.kubernetes_cluster }} -n {{ $labels.resource_namespace }} get events`
  - name: thanos
    rules:
      - alert: ThanosCompactHalted
        expr: thanos_compact_halted{app="thanos-compact",kubernetes_namespace=~"sys-.*"} == 1
        labels:
          team: infra
        annotations:
          summary: Thanos compaction has failed to run and now is halted
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/s48S7j4ik/thanos-compaction?refresh=30s&orgId=1&var-interval=1m&var-namespace={{$labels.kubernetes_namespace}}&var-labelselector=app&var-labelvalue=thanos-compact|link>
          logs: <https://grafana.$ENVIRONMENT.aws.uw.systems/explore?left=["now-1h","now","Loki",{"expr":"{kubernetes_cluster=\"{{$labels.kubernetes_cluster}}\",kubernetes_namespace=\"{{$labels.kubernetes_namespace}}\",kubernetes_pod_name=\"{{$labels.kubernetes_name}}.*\",container=\"thanos-compact\"}"}]|link>
      - alert: ThanosCompactCompactionsFailed
        expr: rate(prometheus_tsdb_compactions_failed_total{app="thanos-compact",kubernetes_namespace=~"sys-.*"}[5m]) > 0
        labels:
          team: infra
        annotations:
          summary: Thanos Compact is failing compaction
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/s48S7j4ik/thanos-compaction?refresh=30s&orgId=1&var-interval=1m&var-namespace={{$labels.kubernetes_namespace}}&var-labelselector=app&var-labelvalue=thanos-compact|link>
          logs: <https://grafana.$ENVIRONMENT.aws.uw.systems/explore?left=["now-1h","now","Loki",{"expr":"{kubernetes_cluster=\"{{$labels.kubernetes_cluster}}\",kubernetes_namespace=\"{{$labels.kubernetes_namespace}}\",kubernetes_pod_name=\"{{$labels.kubernetes_name}}.*\",container=\"thanos-compact\"}"}]|link>
      - alert: ThanosSidecarPrometheusDown
        expr: thanos_sidecar_prometheus_up{name="prometheus",kubernetes_namespace=~"sys-.*"} == 0
        for: 15m
        labels:
          team: infra
        annotations:
          summary: Thanos Sidecar cannot connect to Prometheus
          impact: Prometheus configuration is not being refreshed
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/b19644bfbf0ec1e108027cce268d99f7/thanos-sidecar?orgId=1&var-datasource=default&var-interval=5m&var-kubernetes_name={{$labels.kubernetes_name}}|link>
          logs: <https://grafana.$ENVIRONMENT.aws.uw.systems/explore?left=["now-1h","now","Loki",{"expr":"{kubernetes_cluster=\"{{$labels.kubernetes_cluster}}\",kubernetes_namespace=\"{{$labels.kubernetes_namespace}}\",kubernetes_pod_name=\"{{$labels.kubernetes_name}}.*\",container=\"thanos-sidecar\"}"}]|link>
      - alert: ThanosRuleBadConfig
        expr: min(thanos_rule_config_last_reload_successful{app="thanos-rule",kubernetes_namespace=~"sys-.*"}) == 0
        for: 10m
        labels:
          team: infra
        annotations:
          summary: Thanos Rule failed to load alert config
          impact: On Thanos Rule restart alerts wont be loaded.
          action: Ask in slack for any alert changes and check {{ $labels.kubernetes_pod_name }} pod logs in {{ $labels.kubernetes_namespace}} namespace
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/rjUCNfHmz/thanos-rule?refresh=30s&orgId=1&var-interval=1m&var-namespace={{ $labels.kubernetes_namespace}}&var-labelselector=app&var-labelvalue=thanos-rule|link>
          logs: <https://grafana.$ENVIRONMENT.aws.uw.systems/explore?left=["now-1h","now","Loki",{"expr":"{kubernetes_cluster=\"{{$labels.kubernetes_cluster}}\",kubernetes_namespace=\"{{$labels.kubernetes_namespace}}\",kubernetes_pod_name=\"{{$labels.kubernetes_name}}.*\",container=\"thanos-rule\"}"}]|link>
  - name: external-dns
    rules:
      - alert: ExternalDnsRegistryErrors
        expr: rate(registry_errors_total{app="external-dns"}[5m]) > 0
        for: 15m
        labels:
          team: infra
        annotations:
          description: "{{ $labels.kubernetes_pod_name }} errors while talking to dns registry"
          summary: external-dns registry errors
      - alert: ExternalDnsSourceErrors
        expr: rate(source_errors_total{app="external-dns"}[5m]) > 0
        for: 15m
        labels:
          team: infra
        annotations:
          description: "{{ $labels.kubernetes_pod_name }} errors while talking to kube api"
          summary: external-dns source errors
  - name: etcd
    rules:
      - alert: KubernetesEtcdNodeDown
        expr: up{job="etcd"} != 1
        for: 10m
        labels:
          team: infra
        annotations:
          description: "{{ $labels.instance }} has been down for more than 10 minutes."
          summary: Kubernetes etcd node is down
          impact: "etcd cluster has limited node redundancy."
          action: "Check etcd service status on {{$labels.instance}}."
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/mYdnw3aik/kubernetes-etcd|link>
      - alert: KubernetesEtcdNoLeader
        expr: etcd_server_has_leader{job="etcd"} == 0
        for: 1m
        labels:
          team: infra
        annotations:
          summary: "{{$labels.instance}} has no leader."
          impact: "etcd cluster {{$labels.instance}} is not available."
          action: "Check etcd service status on {{$labels.instance}}."

      ## https://github.com/etcd-io/etcd/issues/10289
      ## Commented out, pending resolution of the issue above
      ##
      ## Currently tagged in "v3.5.0-alpha.0"
      #- alert: KubernetesEtcdHighNumberOfFailedGRPCRequests
      #  expr: 100 * sum(rate(grpc_server_handled_total{grpc_code!="OK",job="etcd"}[5m])) by ( instance, grpc_service,grpc_method) /  sum(rate(grpc_server_handled_total{job="etcd"}[5m])) by (instance, grpc_service, grpc_method) > 1
      #  for: 10m
      #  labels:
      #    team: infra
      #  annotations:
      #    summary: "{{$labels.instance}} etcd has many requests failed last 10min"
      #    impact: "{{$labels.instance}} etcd is returning errors."
      #    action: "Check RPC failed rate on dashboard and {{$labels.instance}} etcd service logs."
      #    dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/mYdnw3aik/kubernetes-etcd|link>

      ## https://github.com/etcd-io/etcd/issues/11100#issuecomment-613776203
      ## > It looks to me like the RAFT_MESSAGE round-tripper is not very
      ## > relevant to performance delivered to clients.
      ##
      ## https://github.com/etcd-io/etcd/issues/10292
      #- alert: KubernetesEtcdMemberCommunicationSlow
      #  expr: histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job="etcd"}[5m])) > 0.2048
      #  for: 10m
      #  labels:
      #    team: infra
      #  annotations:
      #    summary: "{{$labels.instance}} member communication is slow"
      #    impact: "{{$labels.instance}} is responding slowly."
      #    action: "Check {{$labels.instance}} etcd service logs."
      #    dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/mYdnw3aik/kubernetes-etcd|link>
      - alert: KubernetesEtcdHighNumberOfFailedProposals
        expr: rate(etcd_server_proposals_failed_total{job="etcd"}[5m]) > 0
        for: 15m
        labels:
          team: infra
        annotations:
          summary: "{{$labels.instance}} etcd member has a high number of raft proposals failing."
          impact: "{{$labels.instance}} etcd might not be working."
          action: "Check Raft proposals dashboard and {{$labels.instance}} etcd service logs."
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/mYdnw3aik/kubernetes-etcd|link>
      - alert: KubernetesEtcdHighDiskSyncDurations
        expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job="etcd"}[5m])) > 0.5
        for: 10m
        labels:
          team: infra
        annotations:
          summary: "{{$labels.instance}} etcd fsync durations are high"
          impact: "{{$labels.instance}} etcd is responding slowly."
          action: "Check Disk Sync Duration and resources for {{$labels.instance}}."
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/mYdnw3aik/kubernetes-etcd|link>
      - alert: KubernetesEtcdHighCommitDurations
        expr: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job="etcd"}[5m])) > 0.25
        for: 10m
        labels:
          team: infra
        annotations:
          summary: "{{$labels.instance}} etcd commit durations are high"
          impact: "{{$labels.instance}} etcd is responding slowly."
          action: "Check {{$labels.instance}} resources."
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/mYdnw3aik/kubernetes-etcd|link>
      - alert: EtcdBackupJobFailed
        expr: time() - max(kube_job_status_completion_time{namespace="kube-system",job_name=~"etcd-backup-.*"}) > 108000
        labels:
          team: infra
        annotations:
          summary: "Etcd backup jobs have not completed in the last 30h"
          action: "Check cronjob status and job logs "
          logs: <https://grafana.$ENVIRONMENT.aws.uw.systems/explore?left=["now-1h","now","Loki",{"expr":"{kubernetes_cluster=\"{{$labels.kubernetes_cluster}}\",kubernetes_namespace=\"{{$labels.namespace}}\",container=\"{{$labels.container}}\"}"}]|link>
  - name: tls-probe-traefik
    rules:
      - alert: TLSProbeTargetDown
        expr: up{job=~"^tls-cert-check.*$"} != 1
        for: 5m
        labels:
          team: infra
        annotations:
          description: "{{ $labels.instance }} tls probe job reports down more than 5 minutes."
          summary: Traefik instance tcp probe job down
      - alert: TLSProbeFailed
        expr: probe_success{job=~"^tls-cert-check.*$"} == 0
        for: 2m
        labels:
          team: infra
        annotations:
          description: "{{ $labels.instance }} probe fails, check blackbox exporter probes for details (blackbox pods port :9115)"
          summary: Traefik tls probe failed
      - alert: TLSCertExpiringSoon
        expr: probe_ssl_earliest_cert_expiry{job=~"^tls-cert-check.*$"} - time() < 86400 * 28
        labels:
          team: infra
        annotations:
          description: "{{ $labels.instance }} certificate expires in less than 28 days"
          summary: SSL Certificate is due to expire in less than 28 days
  - name: vault
    # Recommendations from https://s3-us-west-2.amazonaws.com/hashicorp-education/whitepapers/Vault/Vault-Consul-Monitoring-Guide.pdf
    rules:
      - alert: VaultHighGCDuration
        expr: increase(vault_runtime_total_gc_pause_ns{kubernetes_namespace="sys-vault"}[1m])/ 1000 > 2000
        for: 10m
        labels:
          team: infra
        annotations:
          description: "{{ $labels.kubernetes_pod_name }} spent more than 2sec/min running GC"
          summary: "{{ $labels.kubernetes_pod_name }} is taking too long to GC"
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/1ysHZE2Wz/vault|link>
      - alert: VaultScarceLeaderContacts
        expr: vault_raft_leader_lastContact{quantile="0.99",kubernetes_namespace="sys-vault"} > 200
        for: 10m
        labels:
          team: infra
        annotations:
          description: "{{ $labels.kubernetes_pod_name }} leader is taking more than 200ms to contact"
          summary: "{{ $labels.kubernetes_pod_name }} contact with leader degraded"
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/1ysHZE2Wz/vault|link>
      # Adapted from https://github.com/giantswarm/vault-exporter/blob/master/vault-mixin/alerts.libsonnet
      - alert: VaultUninitialized
        expr: vault_initialized{kubernetes_namespace="sys-vault"} != 1
        for: 10m
        labels:
          team: infra
        annotations:
          description: "This may indicate an issue with the 'initializer' sidecar"
          summary: "{{ $labels.kubernetes_pod_name }} is uninitialized"
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/1ysHZE2Wz/vault|link>
      - alert: VaultSealed
        expr: vault_sealed{kubernetes_namespace="sys-vault"} != 0
        for: 10m
        labels:
          team: infra
        annotations:
          description: "This may indicate an issue with the 'unsealer' sidecar"
          summary: "{{ $labels.kubernetes_pod_name }} is sealed"
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/1ysHZE2Wz/vault|link>
      - alert: VaultActiveCount
        expr: count(vault_standby{kubernetes_namespace="sys-vault"} == 0) != 1
        for: 10m
        labels:
          team: infra
        annotations:
          description: |
            More or less than 1 active instance typically indicates a problem with leader election.
          summary: "There are {{ $value }} active Vault instance(s)"
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/1ysHZE2Wz/vault|link>
      - alert: VaultUp
        expr: vault_up{kubernetes_namespace="sys-vault"} != 1
        for: 10m
        labels:
          team: infra
        annotations:
          description: |
            The exporter runs as a sidecar and should be able to connect to port 8200 on localhost.
          summary: "Vault exporter for '{{ $labels.kubernetes_pod_name }}' cannot talk to Vault."
          dashboard: <https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/1ysHZE2Wz/vault|link>
      - alert: VaultServerUnreachable
        expr: probe_success{job="vault-server"} == 0
        for: 10m
        labels:
          team: infra
        annotations:
          description: "{{ $labels.instance }} has been down for more than 5 minutes."
          summary: Vault server is not reachable.
      - alert: VaultServerBlackboxTargetDown
        expr: up{job="vault-server"} != 1
        for: 10m
        labels:
          team: infra
        annotations:
          description: "{{ $labels.instance }} http probe job reports down more than 5 minutes."
          summary: Vault server http probe job down
  - name: semaphore
    rules:
      - alert: SemaphorePolicyCalicoClientErrors
        expr: increase(semaphore_policy_calico_client_request_total{success="0"}[5m]) > 0
        for: 5m
        labels:
          team: infra
        annotations:
          summary: "{{ $labels.kubernetes_pod_name }} calico client encountered errors on requests for more than 5 minutes"
          description: GlobalNetworkSets and cross cluster policies may be out of sync due to calico client request failures.
      # According to https://www.wireguard.com/protocol/ handshakes may occur based on `REKEY_AFTER_TIME` and `REJECT_AFTER_TIME` values.
      # Checking on the defaults: https://github.com/WireGuard/wireguard-monolithic-historical/blob/master/src/messages.h seems like
      # the worst case scenario is 5mins between 2 handshakes. Lets triple that and alert if we see that handshaking between peers
      # takes more than 15 minutes. Also, allowing a 5 minute time window before firing to cover for the satrup delay, where the first peer
      # handshake hasn't happened yet and semaphore_wg_peer_last_handshake_seconds is 0.
      - alert: SemaphoreWGPeerLastHandshakeTooFar
        expr: time() - semaphore_wg_peer_last_handshake_seconds > 900
        for: 5m
        labels:
          team: infra
        annotations:
          summary: "wg latest handshake with peer on {{ $labels.device }} device happened more than 15 minutes ago"
          description: "Instance: {{ $labels.instance }} wg latest handshake with peer {{ $labels.public_key }} on {{ $labels.device }} device happened more than 10 minutes ago."
      - alert: SemaphoreWireguardFailedToSyncPeers
        expr: increase(semaphore_wg_sync_peers_total{success="0"}[5m]) > 0
        for: 5m
        labels:
          team: infra
        annotations:
          summary: "{{ $labels.instance }} wg client encountered errors on set peers requests for more than 5 minutes"
          description: WG peers list might be out of sync due to wg client failures.
      - alert: SemaphoreWireguardNodeWatcherErrors
        expr: rate(semaphore_wg_node_watcher_failures_total[5m]) > 0
        for: 10m
        labels:
          team: infra
        annotations:
          summary: "{{ $labels.kubernetes_pod_name }} node watcher to {{ $labels.cluster }} is encountered errors on {{ $labels.verb }} actions for more than 10 minutes"
          description: "Semaphore Wireguard controller fails to {{ $labels.verb }} on cluster {{ $labels.cluster }} node resource."
      - alert: SemaphoreServiceMirrorMismatch
        expr: semaphore_service_mirror_kube_watcher_objects{watcher=~".*-mirror.*"} - ignoring(watcher) semaphore_service_mirror_kube_watcher_objects{watcher!~".*-mirror.*"} != 0
        for: 10m
        labels:
          team: infra
        annotations:
          summary: "{{ $labels.app }}: the number of mirrored {{ $labels.kind }} objects is different to the remote count"
          description: The number of local mirrored objects should match the number of remote objects.
      - alert: SemaphoreServiceMirrorRequeued
        expr: semaphore_service_mirror_queue_requeued_items > 0
        for: 10m
        labels:
          team: infra
        annotations:
          summary: "{{ $labels.app }} has been requeuing {{ $labels.name }} objects for 10 minutes"
          description: |
            Items are requeued when an error is encountered during reconcilliation.

            If requeued items are not being processed promptly then this indicates a persistent issue. The mirror services are likely to be in an incorrect state.
      - alert: SemaphoreServiceMirrorKubeClientErrors
        expr: increase(semaphore_service_mirror_kube_http_request_total{code!="200"}[5m]) > 0
        for: 10m
        labels:
          team: infra
        annotations:
          summary: "{{ $labels.app }} kubernetes client reports errors speaking to apiserver at {{ $labels.host }} for more than 10 minutes"
          description: |
            Kubernetes client requests returning code different than 200 for longer than 10 minutes. Check the pods logs for further information.
          logs: <https://grafana.$ENVIRONMENT.aws.uw.systems/explore?left=["now-1h","now","Loki",{"expr":"{kubernetes_cluster=\"{{$labels.kubernetes_cluster}}\",kubernetes_namespace=\"{{$labels.namespace}}\",container=\"{{$labels.container}}\"}"}]|link>
      - alert: SemaphoreXDSRequeued
        expr: semaphore_xds_queue_requeued_items > 0
        for: 10m
        labels:
          team: infra
        annotations:
          summary: '{{ $labels.app }} has been requeuing {{ $labels.name }} objects for 10 minutes'
          description: |
            Items are requeued when an error is encountered during reconcilliation.

            If requeued items are not being processed promptly then this indicates a persistent issue. The xDS configuration served to clients is likely to be in an incorrect state.
      - alert: SemaphoreXDSKubeClientErrors
        expr: increase(semaphore_xds_kube_http_request_total{code!="200"}[5m]) > 0
        for: 10m
        labels:
          team: infra
        annotations:
          summary: '{{ $labels.app }} kubernetes client reports errors speaking to apiserver at {{ $labels.host }} for more than 10 minutes'
          description: Kubernetes client requests returning code different than 200 for longer than 10 minutes. Check the pods logs for further information.
          logs: <https://grafana.$ENVIRONMENT.aws.uw.systems/explore?left=["now-1h","now","Loki",{"expr":"{kubernetes_cluster=\"{{$labels.kubernetes_cluster}}\",kubernetes_namespace=\"{{$labels.namespace}}\",container=\"{{$labels.container}}\"}"}]|link>
      - alert: SemaphoreXDSNoZoneEndpoint
        expr: sum(semaphore_xds_snapshot_endpoint{locality_zone="none"}) > 0
        for: 1m
        labels:
          team: infra
        annotations:
          summary: '{{ $labels.app }} xDS server snapshot advertises endpoints without a locality zone'
          description: xDS server snapshot contains endpoints with locality zone set to none. This is probably due to a missing zone in the respective Kubernetes EndpointSlice.
          logs: <https://grafana.$ENVIRONMENT.aws.uw.systems/explore?left=["now-1h","now","Loki",{"expr":"{kubernetes_cluster=\"{{$labels.kubernetes_cluster}}\",kubernetes_namespace=\"{{$labels.namespace}}\",container=\"{{$labels.container}}\"}"}]|link>
      - alert: AlertmanagerConfigReloadFailed
        expr: avg(alertmanager_config_last_reload_successful) == 0
        labels:
          team: infra
        annotations:
          summary: "Alertmanager config reload unsuccessful"
          description: "Alertmanager config reload has failed, please check the config is valid"
