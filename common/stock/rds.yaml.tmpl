# PROMETHEUS RULES
# DO NOT REMOVE line above, used in `pre-commit` hook

groups:
  - name: team_detection
    rules:
      # used as: `<metric_with_dbidentifier_label> + on (dbidentifier) group_left (team) uw_rds_owner_team` as the value is always 0
      - record: uw_rds_owner_team
        expr: 'sum by(dbidentifier,team) (label_replace(rds_instance_tags{tag_owner!=""}, "team", "$1", "tag_owner", "(.*)"))'

    #   based on https://github.com/qonto/database-monitoring-framework/blob/main/charts/prometheus-rds-alerts/values.yaml
    #   and AWS https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Best_Practice_Recommended_Alarms_AWS_Services.html#RDS
  - name: RDS
    rules:
      - alert: RDSFreeableMemory
        expr: |
          (max by (aws_account_id, aws_region, dbidentifier) (rds_freeable_memory_bytes{}) * 100 / on(aws_account_id, aws_region, dbidentifier) 
               (max by (instance_class) (rds_instance_memory_bytes{}) * on (instance_class) group_right() max by (aws_account_id, aws_region, dbidentifier, instance_class) (rds_instance_info{})) 
          < 20 ) + on (dbidentifier) group_left (team) uw_rds_owner_team
        for: 10m
        labels:
          alerttype: stock
          alertgroup: RDS
        annotations:
          summary: 'RDS instance {{$labels.dbidentifier}} has {{ printf "%.0f" $value }}% freeable memory'
          impact: "Running out of memory can result in rejected connections"
          link: "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Best_Practice_Recommended_Alarms_AWS_Services.html#RDS"
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-30&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          qonto_runbook: "<https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSMemoryUtilization/|link>"
          action: |
            Low freeable memory means that there is a spike in database connections or that your instance may be under high memory pressure. 
            Check for memory pressure by monitoring the SwapUsage in addition to FreeableMemory. 
            If the instance memory consumption is frequently too high, this indicates that you should check your workload or upgrade your instance class.
            NOTE: `db.t4g.micro` is constantly under memory pressure, so it's usable only for testing that your connection setup works, but it's not suitable for dev usage.
            Consider upgrade at least to `db.t4g.small`.

      - alert: RDSCPUUtilization
        expr: '(max by (aws_account_id, aws_region, dbidentifier) (rds_cpu_usage_percent_average) > 85) + on (dbidentifier) group_left (team) uw_rds_owner_team'
        for: 10m
        labels:
          alerttype: stock
          alertgroup: RDS
        annotations:
          summary: "RDS instance {{$labels.dbidentifier}} has high CPU utilisation"
          impact: "High CPU utilization can lead to very high response time and time-outs"
          link: "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Best_Practice_Recommended_Alarms_AWS_Services.html#RDS"
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-27&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          qonto_runbook: "<https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSCPUUtilization/|link>"
          action: |
            CPU utilization measures non-idle time.
            Consider using Enhanced Monitoring or Performance Insights to review which wait time is consuming the most of the CPU time (guest, irq, wait, nice, and so on). 
            Then evaluate which queries consume the highest amount of CPU. 
            If you can't tune your workload, consider moving to a larger DB instance class.
      # Merged the AWS DBload alert & qonto NonCPUUtilisation into this one.
      # Diverged from the qonto alert by using as a threshold the number of vCPUS, rather than multiplying with 4.
      # Asked a question of why are they multiplying here: https://github.com/qonto/database-monitoring-framework/issues/48
      - alert: RDSDBLoad
        expr: |
          max by (aws_account_id, aws_region, dbidentifier) (rds_dbload_average)
          > on(aws_account_id, aws_region, dbidentifier) (
          max by (instance_class) (rds_instance_vcpu_average{}) * on (instance_class) group_right() max by (aws_account_id, aws_region, dbidentifier, instance_class) (rds_instance_info{})
          ) + on (dbidentifier) group_left (team) uw_rds_owner_team
        for: 10m
        labels:
          alerttype: stock
          alertgroup: RDS
        annotations:
          summary: 'RDS instance {{$labels.dbidentifier}} has high DB load: {{ printf "%.0f" $value }}'
          impact: "Performance is degraded because some queries could not be executed."
          link: "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Best_Practice_Recommended_Alarms_AWS_Services.html#RDS"
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-26&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          qonto_runbook: "https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSNonCPUUtilization/|link>"
          perf_insides_link: "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.Components.html#USER_PerfInsights.UsingDashboard.Components.AvgActiveSessions.dims"
          action: |
            If the number of processes exceed the number of vCPUs, the processes start queuing. When the queuing increases, the performance is impacted. 
            Check the linked dashboard as the are 2 causes:
            1. The DB load is caused by `CPU execution`: you can monitor CPUUtilization, DBLoadCPU and queued tasks in Performance Insights/Enhanced Monitoring. 
               You might want to throttle connections to the instance, tune any SQL queries with a high CPU load, or consider a larger instance class.
            2. The DB load is caused by `Non CPU execution`:             
              This situation usually occurs when SQL queries are blocked by software reason (e.g. Locks) or hardware saturation (e.g. IOPS)
              Identify the waits type of active queries on RDS Performance insights.
              Select `Top waits` and `Top database` to quickly identify wait reason and database.
              Mitigation:
              a. If Lock:relation, identify and fix the lock reason
                 For other wait type, looks at https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/PostgreSQL.Tuning.html
              b. Kill the SQL queries

      - alert: RDSSwapUtilization
        expr: (max by (aws_account_id, aws_region, dbidentifier) (delta(rds_swap_usage_bytes{}[1h])) / 1024 / 1024 >= 20) + on (dbidentifier) group_left (team) uw_rds_owner_team
        for: 2m
        annotations:
          summary: "{{ $labels.dbidentifier }} SWAP utilization is high"
          impact: "Performance could be degraded"
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-32&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          qonto_runbook: "https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSSwapUtilization/|link>"
          action: |
            1. Check memory usage over last weeks to identify if server is missing memory
            2. Check if there long running PostgreSQL clients that donâ€™t execute SQL queries for a while on live dashboard:
              ```SQL
              SELECT
                pid,
                usename,
                datname,
                application_name,
                age(now(), query_start) as last_query_age,
                age(now(), backend_start) as backend_age,
                backend_start,
                query_start last_query,
                left(query, 60) query
              FROM pg_stat_activity
              WHERE query_start is not null
              AND pid != pg_backend_pid()
              AND usename != 'rdsrepladmin'
              AND state != 'active'
              AND query not like 'START_REPLICATION %'
              AND query_start < NOW() - INTERVAL '24 HOURS'
              ORDER by query_start asc;
              ```
            Mitigation
            1. Reduce number of concurrent connections on the server
            2. Increase RDS instance type to have more memory

      - alert: RDSIOPSUtilization
        expr: (max by (aws_account_id, aws_region, dbidentifier) ((rds_read_iops_average{} + rds_write_iops_average{}) * 100 / rds_max_disk_iops_average{}) > 80)  + on (dbidentifier) group_left (team) uw_rds_owner_team
        for: 10m
        annotations:
          summary: '{{ $labels.dbidentifier }} uses {{ printf "%.0f" $value }}% of its disk IOPS'
          impact: "Performance could be degraded"
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-29&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          qonto_runbook: "https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSIOPSUtilization/|link>"
          action: |
            Open RDS Performance insights to identify IOPS-intensive queries.
            Mitigation:
              1. Kill SQL queries that generate intensive IOPS
              2. Try to improve the query
              3. Increase provisioned IOPS if possible (gp3 or io1 storage class). Be aware that some instances have IOPS limits.
