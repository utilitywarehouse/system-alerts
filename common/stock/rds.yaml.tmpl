# PROMETHEUS RULES
# DO NOT REMOVE line above, used in `pre-commit` hook

groups:
  - name: team_detection
    rules:
      # used as: `<metric_with_dbidentifier_label> + on (dbidentifier) group_left (team) uw_rds_owner_team` as the value is always 0
      - record: uw_rds_owner_team
        expr: 'sum by(dbidentifier,team) (label_replace(rds_instance_tags{tag_owner!=""}, "team", "$1", "tag_owner", "(.*)"))'

    #   based on https://github.com/qonto/database-monitoring-framework/blob/main/charts/prometheus-rds-alerts/values.yaml
    #   and AWS https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Best_Practice_Recommended_Alarms_AWS_Services.html#RDS
  - name: RDS
    rules:
      - alert: RDSFreeableMemory
        expr: |
          (max by (aws_account_id, aws_region, dbidentifier) (rds_freeable_memory_bytes{}) * 100 / on(aws_account_id, aws_region, dbidentifier) 
               (max by (instance_class) (rds_instance_memory_bytes{}) * on (instance_class) group_right() max by (aws_account_id, aws_region, dbidentifier, instance_class) (rds_instance_info{})) 
          < 25 ) + on (dbidentifier) group_left (team) uw_rds_owner_team
        for: 15m
        labels:
          alerttype: stock
          alertgroup: RDS
        annotations:
          summary: 'RDS instance {{$labels.dbidentifier}} has {{ printf "%.0f" $value }}% freeable memory'
          impact: "Running out of memory can result in rejected connections"
          link: "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Best_Practice_Recommended_Alarms_AWS_Services.html#RDS"
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-30&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          qonto_runbook: "<https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSMemoryUtilization/|link>"
          action: |
            Low freeable memory means that there is a spike in database connections or that your instance may be under high memory pressure. 
            Check for memory pressure by monitoring the SwapUsage in addition to FreeableMemory. 
            If the instance memory consumption is frequently too high, this indicates that you should check your workload or upgrade your instance class.
            NOTE: `db.t4g.micro` is constantly under memory pressure, so it's usable only for testing that your connection setup works, but it's not suitable for dev usage.
            Consider upgrade at least to `db.t4g.small`.

      - alert: RDSCPUUtilization
        expr: '(max by (aws_account_id, aws_region, dbidentifier) (rds_cpu_usage_percent_average) > 85) + on (dbidentifier) group_left (team) uw_rds_owner_team'
        for: 10m
        labels:
          alerttype: stock
          alertgroup: RDS
        annotations:
          summary: "RDS instance {{$labels.dbidentifier}} has high CPU utilisation"
          impact: "High CPU utilization can lead to very high response time and time-outs"
          link: "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Best_Practice_Recommended_Alarms_AWS_Services.html#RDS"
          dashboard: "<https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/a7049b32-6be3-42e5-aa9a-2879a14f46dd/rds-instance?orgId=1&refresh=1m&from=now-12h&to=now&viewPanel=panel-27&var-dbidentifier={{ $labels.dbidentifier }}|link>"
          qonto_runbook: "<https://qonto.github.io/database-monitoring-framework/latest/runbooks/rds/RDSCPUUtilization/|link>"
          action: |
            CPU utilization measures non-idle time.
            Consider using Enhanced Monitoring or Performance Insights to review which wait time is consuming the most of the CPU time (guest, irq, wait, nice, and so on). 
            Then evaluate which queries consume the highest amount of CPU. 
            If you can't tune your workload, consider moving to a larger DB instance class.
