# PROMETHEUS RULES
# DO NOT REMOVE line above, used in `pre-commit` hook

groups:
  - name: logging
    rules:
      - alert: LogForwarderIsDown(external)
        expr: 'up{job="log-forwarder"} < 1'
        for: 30m
        labels:
          team: infra
        annotations:
          summary: "{{ $labels.instance }} log forwarder is not exposing metrics for 30m"
          action: "ssh into {{ $labels.instance }} and make sure `promtail.service` is running"
      - alert: PromtailThrottling
        expr: 'label_replace(rate(logentry_dropped_lines_by_label_total{label_name="limit_key", label_value=~"kube-system.*|sys-.*"}[5m]) > 10, "label_namespace", "$1", "label_value", `(.*)\/(.*)`)'
        for: 10m
        labels:
          team: infra
        annotations:
          summary: "{{ $labels.label_value }} is throttling and dropping logs"
          dashboard: '<https://grafana.$ENVIRONMENT.aws.uw.systems/explore?left=["now-6h","now","Loki",{"expr":"sum(count_over_time({kubernetes_cluster=\"{{$labels.kubernetes_cluster}}\",kubernetes_namespace=\"{{$labels.label_namespace}}\"}[5m]))by(container)"}]|link>'
      - alert: PromtailDroppingSystemLogs(external)
        expr: 'rate(promtail_dropped_entries_total{reason="ingester_error"}[5m]) > 0'
        for: 10m
        labels:
          team: infra
        annotations:
          summary: "{{ $labels.instance }} is being noisy and dropping logs"
      - alert: VectorFailingToSendToLoki
        # Checking that buffers are full (close to the max 500 events). Note
        # that buffers won't get full unless logs are not getting through at
        # all, since vector will throttle the inputs to keep the buffers in a
        # comfortable state
        expr: 'vector_buffer_events{component_id="loki"} > 475'
        for: 5m
        labels:
          team: infra
        annotations:
          summary: "Vector cannot talk to loki"
          description: "Vector's loki buffer is full, which means that it cannot talk to loki"
          pod: "{{ $labels.kubernetes_pod_name }}"
          impact: "Vector is not accepting new logs from it's input sources"
          action: "Check if loki is up and if vector can talk to it"
          dashboard: "https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/bdnuhz796molca/vector?var-pod={{ $labels.kubernetes_pod_name }}"
          dashboard_fallback: "https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/bdnuhz796molca/vector"
      # Main alert, intended for "high rate" inputs
      # kubernetes_logs cardinality is huge and is causing this alert to fail
      - alert: VectorFailingToInput2h
        expr: 'sum by (component_id) (rate(vector_component_received_events_total{component_kind="source",component_id!~"kubernetes_logs|kubernetes_events|s3_msk|uw_link_firewall_events|careers_uw_co_uk_firewall_events|myaccount_uw_co_uk_firewall_events|uw_partners_firewall_events|dev_merit_uw_systems_firewall_events|gcp_audits|github_audits"}[5m])) == 0'
        for: 2h
        labels:
          team: infra
        annotations:
          summary: "Vector is not receiving logs"
          description: "Vector received no logs from {{ $labels.component_id }} for 2h"
          dashboard: "https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/bdnuhz796molca/vector"
