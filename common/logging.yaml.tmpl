# PROMETHEUS RULES
# DO NOT REMOVE line above, used in `pre-commit` hook

groups:
  - name: logging
    rules:
      - alert: LogForwarderIsDown(external)
        expr: 'up{job="log-forwarder"} < 1'
        for: 30m
        labels:
          team: infra
        annotations:
          summary: "{{ $labels.instance }} log forwarder is not exposing metrics for 30m"
          action: "ssh into {{ $labels.instance }} and make sure `vector.service` is running"
      - alert: VectorFailingToSendToLoki
        # Checking that buffers are full (close to the max 500 events). Note
        # that buffers won't get full unless logs are not getting through at
        # all, since vector will throttle the inputs to keep the buffers in a
        # comfortable state
        expr: 'vector_buffer_events{component_id="loki"} > 475'
        for: 5m
        labels:
          team: infra
        annotations:
          summary: "Vector cannot talk to loki"
          description: "Vector's loki buffer is full, which means that it cannot talk to loki"
          pod: "{{ $labels.kubernetes_pod_name }}"
          impact: "Vector is not accepting new logs from it's input sources"
          action: "Check if loki is up and if vector can talk to it"
          dashboard: "https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/bdnuhz796molca/vector?var-pod={{ $labels.kubernetes_pod_name }}"
          dashboard_fallback: "https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/bdnuhz796molca/vector"
      # Main alert, intended for "high rate" inputs
      # kubernetes_logs cardinality is huge and is causing this alert to fail
      - alert: VectorFailingToInput2h
        expr: 'sum by (component_id) (rate(vector_component_received_events_total{component_kind="source",component_id!~"kubernetes_logs|kubernetes_events|s3_msk|uw_link_firewall_events|careers_uw_co_uk_firewall_events|myaccount_uw_co_uk_firewall_events|uw_partners_firewall_events|dev_merit_uw_systems_firewall_events|gcp_audits|github_audits"}[5m])) == 0'
        for: 2h
        labels:
          team: infra
        annotations:
          summary: "Vector is not receiving logs"
          description: "Vector received no logs from {{ $labels.component_id }} for 2h"
          dashboard: "https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/bdnuhz796molca/vector"
      - alert: VectorThrottlingTooManyLogs
        expr: rate(vector_component_discarded_events_total{component_type="throttle"}[10m])> 5000
        for: 1h
        labels:
          team: infra
        annotations:
          summary: "Vector is being overwhelmed by noisy logs (>5000 log/s for 1h)"
          description: "Logs are being throttled, but vector may be taken down by the load"
          pod: "{{ $labels.kubernetes_pod_name }}"
          impact: "Vector functionality may be degraded due to load"
          action: "Check dashboard to find the problematic pod (`Rate Limit Details` row), add it to the VECTOR_DROP_PATTERN kill-switch, and notify owners"
          dashboard: "https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/bdnuhz796molca/vector?var-pod={{ $labels.kubernetes_pod_name }}"
          dashboard_fallback: "https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/bdnuhz796molca/vector"
